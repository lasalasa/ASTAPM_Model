{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7153a659",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_core_utils.ipynb\n",
    "# %run notebook_text_processor.ipynb\n",
    "%run notebook_core_mappers.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f57c33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# from .core_mappers import HFACS_DICTIONARY, AUTO_LABELING_DICTIONARY\n",
    "# from .core_utils import CoreUtils\n",
    "\n",
    "def get_dic(ds_name, is_imbalance):\n",
    "    if is_imbalance:\n",
    "        return [\n",
    "            HFACS_DICTIONARY['hfacs_balance'],\n",
    "            AUTO_LABELING_DICTIONARY[f'{ds_name}_balance']\n",
    "        ]\n",
    "    else:\n",
    "        return [\n",
    "            HFACS_DICTIONARY['hfacs'],\n",
    "            AUTO_LABELING_DICTIONARY[ds_name]\n",
    "        ]\n",
    "\n",
    "class AutoLabeling:\n",
    "\n",
    "    def __init__(self, df, ds_name='asrs', is_imbalance=False):\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"FACTOR_COL_NAME\"]\n",
    "        hfacs_dic, auto_label_dic = get_dic(ds_name, is_imbalance)\n",
    "        \n",
    "        self.factor_col_name = factor_col_name\n",
    "        self.hfacs_dic = hfacs_dic\n",
    "        self.auto_label_dic = auto_label_dic\n",
    "        self.df = df\n",
    "\n",
    "    def __map_hf_asrs(self, factors):\n",
    "        hf_dic = self.auto_label_dic['HF_DICTIONARY']\n",
    "\n",
    "        # Convert to lowercase for consistent keyword matching\n",
    "        factors = str(factors).lower()\n",
    "\n",
    "        split_factors = factors.split(':')\n",
    "        if(len(split_factors) == 1):\n",
    "            # Only include Human factor, map with # Skill-based Errors TODO Need to improve\n",
    "            return 111 #112\n",
    "        \n",
    "        for keyword, mapping in hf_dic.items():\n",
    "            # print(keyword.lower(), '=>>>', factors)\n",
    "            if keyword.lower() in factors:  # Check if keyword is in the combined issues string\n",
    "                # print(keyword.lower(), '=>>>', factors)\n",
    "                return mapping\n",
    "        return -1\n",
    "\n",
    "    # Function to map combined issues to HFACS based on keyword mappings\n",
    "    def __map_hfacs(self, factors):\n",
    "\n",
    "        pf_dic = self.auto_label_dic\n",
    "\n",
    "        # Convert to lowercase for consistent keyword matching\n",
    "        factors = str(factors).lower()\n",
    "\n",
    "        for keyword, mapping in pf_dic.items():\n",
    "            if keyword.lower() in factors:  # Check if keyword is in the combined issues string\n",
    "                # print(keyword.lower(), '=>>>', factors)\n",
    "                # if(mapping == 11):\n",
    "                #     # print(mapping)\n",
    "                #     return self.__map_hf_asrs(factors)\n",
    "                return mapping\n",
    "        return -1  # Default to unmapped if no match found\n",
    "    \n",
    "    # Function to map combined issues to HFACS based on keyword mappings\n",
    "    # def __map_hfacs_ntsb(self, factors):\n",
    "\n",
    "    #     pf_dic = self.auto_label_dic['PF_DICTIONARY']\n",
    "\n",
    "    #     # Convert to lowercase for consistent keyword matching\n",
    "    #     factors = str(factors).lower()\n",
    "\n",
    "    #     for keyword, mapping in keyword_mappings_ntsb.items():\n",
    "    #         # Check if keyword is in the factors string\n",
    "    #         if keyword.lower() in factors:\n",
    "    #             # print(keyword.lower(), '=>>>', factors)\n",
    "    #             return mapping\n",
    "    #     # Default to unmapped if no match found\n",
    "    #     return -1\n",
    "\n",
    "    def do_auto_label(self, sample_size=0):\n",
    "\n",
    "        # df_tail = df.tail(sample_size).copy()\n",
    "\n",
    "        factor_col_name = self.factor_col_name\n",
    "        df = self.df\n",
    "        hfacs_dic = self.hfacs_dic\n",
    "\n",
    "        if sample_size > 0:\n",
    "            df_tail =  df.sample(n=sample_size, random_state=42)\n",
    "        else:\n",
    "            df_tail = df\n",
    "            \n",
    "        print('sample_size=', df_tail.shape)\n",
    "        \n",
    "        df_tail.replace('', pd.NA, inplace=True)\n",
    "        # print_column = ['ACN'] + factors_column\n",
    "\n",
    "        # df_tail[print_column].value_counts()\n",
    "        # df_tail.isnull().sum()\n",
    "\n",
    "        # Load the dataset (update the file path as needed)\n",
    "        data = df_tail.copy()\n",
    "        # Apply the function to each 'Combined_Issues' row and map to HFACS\n",
    "        data['HFACS_Category_Index'] = data[factor_col_name].apply(self.__map_hfacs)\n",
    "\n",
    "        # Split the 'HFACS_Mapped' into two new columns: 'HFACS_Category' and 'HFACS_Level'\n",
    "        data['HFACS_Category_Value'] = data['HFACS_Category_Index'].apply(lambda x: hfacs_dic[x][3])\n",
    "        data['HFACS_Level'] = data['HFACS_Category_Index'].apply(lambda x: hfacs_dic[x][0])\n",
    "\n",
    "        # # Summarize the mapped HFACS categories and levels\n",
    "        hfacs_summary = data.groupby(['HFACS_Category_Value']).size().reset_index(name='Count')\n",
    "        print(hfacs_summary)\n",
    "        return data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
