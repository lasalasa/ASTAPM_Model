{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b14cbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_core_utils.ipynb\n",
    "# %run notebook_text_processor.ipynb\n",
    "%run notebook_labeling_auto.ipynb\n",
    "%run notebook_model_ls.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "10b402ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "def show_label_distribution(data):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.countplot(data['HFACS_Category_Value_Predict'])\n",
    "    plt.title('The distribution of Primary problem')\n",
    "\n",
    "def word_count_distribution(df):\n",
    "    # Assuming you have a list of all words in your dataset\n",
    "    all_words = [word for text in df['narrative'].values for word in text.split()]\n",
    "    word_counts = Counter(all_words)\n",
    "\n",
    "    # Plot top 50 most frequent words\n",
    "    common_words = word_counts.most_common(50)\n",
    "    labels, values = zip(*common_words)\n",
    "    plt.bar(labels, values)\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.show()\n",
    "\n",
    "    # Print the number of unique words in your dataset\n",
    "    print(f\"Total unique words: {len(word_counts)}\")\n",
    "\n",
    "def show_narrative_distribution(data):\n",
    "    plt.figure(figsize=(14, 6))\n",
    "\n",
    "    word_count = data['narrative_word_count']\n",
    "\n",
    "    sns.histplot(word_count,  bins=50, color='blue', kde=True)\n",
    "    plt.xlabel('Number of Words')\n",
    "    plt.ylabel('Number of sample')\n",
    "    plt.title('Distribution of Word Counts (KDE)')\n",
    "\n",
    "    # Overlay mean (mu) and standard deviation (sigma) on the plot\n",
    "    mean = word_count.mean()\n",
    "    std = word_count.std()\n",
    "    # Display mean and standard deviation on the plot\n",
    "    plt.text(0.7, 0.9, r'$\\mu={:.2f}$'.format(mean), transform=plt.gca().transAxes, fontsize=12)\n",
    "    plt.text(0.7, 0.85, r'$\\sigma={:.2f}$'.format(std), transform=plt.gca().transAxes, fontsize=12)\n",
    "\n",
    "    # Display the plots\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17\n",
    "\n",
    "def model_summary(history):\n",
    "\n",
    "    # Create subplots: 1 row, 2 columns\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "    # Plot the loss on the first axes\n",
    "    axes[0].plot(history.history['loss'], label='train')\n",
    "    axes[0].plot(history.history['val_loss'], label='test')\n",
    "    axes[0].set_title('Loss')\n",
    "    axes[0].set_xlabel('Epochs')\n",
    "    axes[0].set_ylabel('Loss')\n",
    "    axes[0].legend()\n",
    "\n",
    "    # Plot the accuracy on the second axes\n",
    "    axes[1].plot(history.history['accuracy'], label='train')\n",
    "    axes[1].plot(history.history['val_accuracy'], label='test')\n",
    "    axes[1].set_title('Accuracy')\n",
    "    axes[1].set_xlabel('Epochs')\n",
    "    axes[1].set_ylabel('Accuracy')\n",
    "    axes[1].legend()\n",
    "\n",
    "    # Adjust layout to prevent overlap\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "#  Confusion Matrix\n",
    "def show_lstm_confusion_matrix(conf_matrix, classes):\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=classes, yticklabels=classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43cd5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-25 14:11:56.753465: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Text pre-processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.over_sampling import ADASYN\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "\n",
    "# from src.core.core_utils import CoreUtils\n",
    "# from src.core.text_processor import TextPreprocessor\n",
    "# from src.core.model_ls import ModelLS\n",
    "\n",
    "# This should be the same as the 'num_words' in the tokenizer\n",
    "MAX_NB_WORDS = 7500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "def pre_process_df(df, col_name):\n",
    "\n",
    "    textPreprocessor = TextPreprocessor()\n",
    "     # Step 1: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    print(\"combined_narrative\")\n",
    "    df = textPreprocessor.combined_narrative(df)\n",
    "    \n",
    "    # print(\"counting_narrative\")\n",
    "    # # Step 2: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    # df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    # print(\"clean_narrative\")\n",
    "    # # Step 3: clean narrative\n",
    "    # df = textPreprocessor.clean_narrative(df)\n",
    "\n",
    "    print(\"clean_feature\")\n",
    "    # Step 4: Clean factor_column_name\n",
    "    df = textPreprocessor.clean_feature(df, col_name)\n",
    "\n",
    "    print(\"drop_narratives\")\n",
    "    # Step 5: Drop column narrative 01 and 02\n",
    "    df = textPreprocessor.drop_narratives(df)\n",
    "\n",
    "    print(\"preprocess_narrative\")\n",
    "    df = textPreprocessor.preprocess_narrative(df)\n",
    "    df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    # Step 6: Show summary\n",
    "    textPreprocessor.show_summary(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, dfs=None, ds_name='asrs', options={\n",
    "        \"ls_name\": 'asrs_ntsb',\n",
    "        \"ls_version\": 1,\n",
    "        \"sample_size\":1000,\n",
    "        \"max_length\":300, \n",
    "        \"max_nb_words\":5000, \n",
    "        \"is_enable_smote\":False\n",
    "    }):\n",
    "        self.ds_name = ds_name\n",
    "        self.options = options\n",
    "        self.sample_size = options['sample_size']\n",
    "\n",
    "        max_length = options['max_length']\n",
    "        self.max_length = max_length\n",
    "\n",
    "        max_nb_words = options['max_nb_words']\n",
    "        self.max_nb_words  = max_nb_words \n",
    "        self.is_enable_smote = options['is_enable_smote']\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"LS_CLASSIFICATION_FACTOR\"]\n",
    "        self.factor_col_name = factor_col_name\n",
    "\n",
    "        if dfs is None:\n",
    "            print(\"The DataFrame is None\")\n",
    "            dfs = self.get_data()        \n",
    "            print(\"Data loaded\")\n",
    "        \n",
    "        # print(dfs[ds_name])\n",
    "\n",
    "        dfs = self.pre_process(dfs)\n",
    "        self.dfs = dfs\n",
    "        print(\"Pre processed\")\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        show_label_distribution(df)\n",
    "        show_narrative_distribution(df)\n",
    "        word_count_distribution(df)\n",
    "\n",
    "        print(\"Define Y\")\n",
    "        lstm_labels = df['HFACS_Category_Value_Predict'].values\n",
    "        lstm_label_encoder = LabelEncoder()\n",
    "        Y = lstm_label_encoder.fit_transform(lstm_labels)\n",
    "        self.Y = Y\n",
    "        self.label_encoder = lstm_label_encoder\n",
    "        print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "        print(\"Define X\")\n",
    "        texts = df['narrative'].values\n",
    "        tokenizer = Tokenizer(num_words=max_nb_words, oov_token=\"<OOV>\")\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=max_length)\n",
    "        self.X = X\n",
    "\n",
    "        print('Shape of data tensor:', X.shape)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(max_nb_words, EMBEDDING_DIM))\n",
    "        model.add(SpatialDropout1D(0.25))\n",
    "        model.add(LSTM(units=64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))\n",
    "        model.add(LSTM(units=64, dropout=0.25, recurrent_dropout=0.25))\n",
    "        model.add(Dense(len(lstm_label_encoder.classes_), activation='softmax', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0001),  metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def get_data(self):\n",
    "        ds_name = self.ds_name\n",
    "        ds_name_list = ds_name.split('_')\n",
    "\n",
    "        # print(ds_name_list)\n",
    "        self.sample_size = self.sample_size * len(ds_name_list)\n",
    "\n",
    "        dfs = {}\n",
    "        dfs_list = []\n",
    "        for ds_name_item in ds_name_list:\n",
    "            df_item = CoreUtils.get_data(ds_name_item)\n",
    "            dfs[ds_name_item] = df_item\n",
    "            dfs_list.append(df_item)\n",
    "\n",
    "        dfs[ds_name] = pd.concat(dfs_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "        return dfs\n",
    "\n",
    "    def define_hyperparameters(self, options):\n",
    "        # Hyperparameters \n",
    "        self.max_nb_words = 10000 # max number of words to use in the vocabulary\n",
    "        self.max_length = 100 # max length of each text (in terms of number of words)\n",
    "        self.embedding_dim = 100 # dimension of word embeddings\n",
    "        self.lstm_units = 64 # number of units in the LSTM layer \n",
    "        self.num_of_class = len(self.lstm_label_encoder.classes_)\n",
    "    \n",
    "    def define_model(self):\n",
    "        model = Sequential()\n",
    "        model.add(Embedding(self.max_nb_words, self.embedding_dim))\n",
    "        model.add(SpatialDropout1D(0.25))\n",
    "        model.add(LSTM(units=self.lstm_units, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))\n",
    "        model.add(LSTM(units=64, dropout=0.25, recurrent_dropout=0.25))\n",
    "        model.add(Dense(len(self.num_of_class), activation='softmax', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "    def pre_process(self, dfs):\n",
    "        ds_name = self.ds_name\n",
    "        sample_size = self.sample_size\n",
    "        factor_col_name = self.factor_col_name\n",
    "        options = self.options\n",
    "\n",
    "        ls_name = options['ls_name']\n",
    "        ls_version = options['ls_version']\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        # 02. Label Spreading\n",
    "        print(\"Start labelling\")\n",
    "        df = ModelLS.predict(df, ls_name, ls_version, sample_size)\n",
    "        print(\"Ladled Sampling size=\", df.shape)\n",
    "\n",
    "        print(\"start pre_process_df\")\n",
    "        df = pre_process_df(df, factor_col_name)\n",
    "\n",
    "        dfs[ds_name] = df\n",
    "\n",
    "        # print(df.head())\n",
    "        return dfs\n",
    "\n",
    "\n",
    "    def train(self, epochs=10, batch_size=32):\n",
    "\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "        is_enable_smote = self.is_enable_smote\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.3, random_state = 42)\n",
    "        \n",
    "        print(X_train.shape,Y_train.shape)\n",
    "        print(X_test.shape,Y_test.shape)\n",
    "\n",
    "        # print(pd.Series(Y_train).value_counts())\n",
    "        print(pd.Series(Y_test).value_counts())\n",
    "        \n",
    "        if is_enable_smote:\n",
    "\n",
    "            smote = SMOTE(random_state=42)\n",
    "            X_resampled, Y_resampled = smote.fit_resample(X_train, Y_train)\n",
    "\n",
    "            print(f\"Original dataset shape: {X_train.shape}\")\n",
    "            print(f\"Resampled dataset shape: {X_resampled.shape}\")\n",
    "\n",
    "        else: \n",
    "            X_resampled = X_train\n",
    "            Y_resampled = Y_train\n",
    "\n",
    "\n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "            patience=2, \n",
    "            min_delta=0.001)\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(X_resampled, Y_resampled, \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1, \n",
    "            callbacks=[early_stopping])\n",
    "        \n",
    "        self.history = history\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "\n",
    "        self.save()\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Predict the probabilities for the test set\n",
    "        model = self.model\n",
    "        ds_name = self.ds_name\n",
    "        X = self.X\n",
    "        Y_test = self.Y_test\n",
    "        X_test = self.X_test\n",
    "        labels = self.label_encoder.classes_\n",
    "\n",
    "        map_labels = self.label_encoder.fit_transform(labels)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Convert probabilities to class indices\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "        self.Y_pred = y_pred_classes\n",
    "\n",
    "        conf_matrix = confusion_matrix(Y_test, y_pred_classes, labels=map_labels)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(Y_test, y_pred_classes)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        report = classification_report(Y_test, y_pred_classes, labels=map_labels, output_dict=True)\n",
    "\n",
    "        # Classification report for precision, recall, F1-score\n",
    "        # print(classification_report(Y_test, y_pred_classes, labels=map_labels))\n",
    "\n",
    "        # print(conf_matrix)\n",
    "\n",
    "        history = self.history\n",
    "\n",
    "        train_loss = history.history['loss']\n",
    "        test_loss = history.history['val_loss']\n",
    "\n",
    "        train_accuracy = history.history['accuracy']\n",
    "        test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "        # print(train_loss, test_loss)\n",
    "\n",
    "        df = self.dfs[ds_name]\n",
    "\n",
    "        train_data_label_value_count = df['HFACS_Category_Value_Predict'].value_counts().reset_index()\n",
    "\n",
    "        # Convert DataFrame to dictionary or list of records\n",
    "        train_data_label_value_count_dict = train_data_label_value_count.to_dict(orient='records')\n",
    "\n",
    "        # https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "        all_words = [word for text in df['narrative'].values for word in text.split()]\n",
    "        word_counts = Counter(all_words)\n",
    "        common_words = word_counts.most_common(50)\n",
    "        word_count_labels, word_count_values = zip(*common_words)\n",
    "\n",
    "        narrative_word_count = df['narrative_word_count'].tolist()\n",
    "\n",
    "        predict_text = model.predict(X)\n",
    "        # Convert probabilities to class indices\n",
    "        predict_text_classes = np.argmax(predict_text, axis=1)\n",
    "\n",
    "        predict_text_label = self.label_encoder.inverse_transform(predict_text_classes)\n",
    "\n",
    "        df['lstm_predict_label'] = predict_text_label\n",
    "\n",
    "        # print(df['date'])\n",
    "        \n",
    "        new_df = df.groupby('date')['lstm_predict_label'].value_counts().unstack().fillna(0)\n",
    "\n",
    "        model_summary(history)\n",
    "        show_lstm_confusion_matrix(conf_matrix, labels)\n",
    "\n",
    "        evaluation_result = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report,\n",
    "            \"conf_matrix\": conf_matrix.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"train_label_value_count\": train_data_label_value_count_dict,\n",
    "            \"train_word_count\": {\n",
    "                \"labels\": word_count_labels,\n",
    "                \"values\": word_count_values\n",
    "            },\n",
    "            \"train_narrative_word_count\": narrative_word_count,\n",
    "            \"sample_predict_view\": new_df.to_dict(orient=\"index\")\n",
    "        }\n",
    "\n",
    "        # print(evaluation_result)\n",
    "\n",
    "        return evaluation_result\n",
    "\n",
    "    def predict(self, text):\n",
    "        loaded_model = self.load()\n",
    "        predict_text = loaded_model.predict(text)\n",
    "        predict_text_classes = np.argmax(predict_text, axis=1)\n",
    "        return predict_text_classes\n",
    "    \n",
    "    def save(self):\n",
    "        ds_name = self.ds_name\n",
    "        self.model.save(f'{ds_name}.keras')\n",
    "    \n",
    "    def load(self):\n",
    "        ds_name = self.ds_name\n",
    "        loaded_model = load_model(f'{ds_name}.keras')\n",
    "        loaded_model.summary()\n",
    "        return loaded_model\n",
    "\n",
    "# https://www.kaggle.com/code/khotijahs1/using-lstm-for-nlp-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
