{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "41339f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lasanthalakmal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lasanthalakmal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import re\n",
    "import nltk\n",
    "# from nltk.corpus import stopwords\n",
    "# from nltk.tokenize import word_tokenize\n",
    "# from gensim.models import Word2Vec\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#library that contains punctuation\n",
    "import string\n",
    "string.punctuation\n",
    "import pandas as pd\n",
    "\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#importing nlp library\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "#importing the Stemming function from nltk library\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, acronyms=None):\n",
    "        self.porter_stemmer = PorterStemmer()\n",
    "        #defining the object for Lemmatization\n",
    "        self.wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def combined_narrative(self, df):\n",
    "        df['narrative_02'] = df['narrative_02'].fillna('')\n",
    "\n",
    "        # Step 4: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "        # code adapted from GeeksforGeeks (2021)\n",
    "        df['narrative'] = df['narrative_01'].str.cat(df['narrative_02'], sep=' ', na_rep='')\n",
    "        # end of adapted code\n",
    "        return df\n",
    "\n",
    "    def counting_narrative(self, df):\n",
    "        df['narrative_length'] = df['narrative'].apply(len)\n",
    "        # Count words in each narrative\n",
    "        df['narrative_word_count'] = df['narrative'].apply(lambda x: len(x.split()))\n",
    "\n",
    "        # Count sentences in each narrative\n",
    "        df['narrative_sentence_count'] = df['narrative'].apply(lambda x: len(x.split('. ')))\n",
    "        \n",
    "        return df\n",
    "\n",
    "    # def clean_narrative(self, df):\n",
    "    #     df.dropna(subset=['narrative'], inplace=True)\n",
    "\n",
    "    #     df = df[df['narrative_word_count'] > 0].copy()\n",
    "    #     return df\n",
    "\n",
    "    def clean_feature(self, df: pd.DataFrame, feature_name):\n",
    "        df = df.dropna(subset=[feature_name])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def drop_narratives(self, df):\n",
    "        # Drop column narrative 01 and 02\n",
    "        df = df.drop('narrative_01', axis=1)\n",
    "        df = df.drop('narrative_02', axis=1)\n",
    "        return df\n",
    "\n",
    "    def show_summary(self, df: pd.DataFrame):\n",
    "        null_summary = df.isnull().sum()\n",
    "        print(null_summary)\n",
    "        print(df.head())\n",
    "\n",
    "    ## -------------- NLP Preprocessing\n",
    "\n",
    "    # code adapted from Deepanshi (2024)\n",
    "    #defining the function to remove punctuation\n",
    "    def remove_punctuation(self, text):\n",
    "        punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
    "        return punctuationfree\n",
    "\n",
    "    def do_lower(self, df, col_name):\n",
    "        df[col_name]= df[col_name].apply(lambda x: x.lower())\n",
    "        return df\n",
    "\n",
    "    # code adapted from (NLTKâ€¯:: Nltk.Tokenize Package, n.d.)\n",
    "    def tokenization(self, text):\n",
    "        return word_tokenize(text)\n",
    "    # end of adapted code\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        output = [word for word in text if word not in stop_words]\n",
    "        return output\n",
    "\n",
    "    #defining a function for stemming\n",
    "    def stemming(self, text):\n",
    "        #defining the object for stemming\n",
    "        stem_text = [self.porter_stemmer.stem(word) for word in text]\n",
    "        return stem_text\n",
    "\n",
    "    #defining the function for lemmatization\n",
    "    def lemmatizer(self, text):\n",
    "        lemm_text = [self.wordnet_lemmatizer.lemmatize(word) for word in text]\n",
    "        return lemm_text\n",
    "    # end of adapted code\n",
    "    \n",
    "    def preprocess_narrative(self, df):\n",
    "        col_name = 'narrative' #'narrative_cleaned'\n",
    "        df[col_name]= df[col_name].apply(lambda x: self.remove_punctuation(x))\n",
    "\n",
    "        df = self.do_lower(df, col_name)\n",
    "\n",
    "        df[col_name] = df[col_name].apply(lambda x: self.tokenization(x))\n",
    "\n",
    "        df[col_name] = df[col_name].apply(lambda x: self.remove_stopwords(x))\n",
    "\n",
    "        df[col_name] = df[col_name].apply(lambda x: self.stemming(x))\n",
    "\n",
    "        df[col_name] = df[col_name].apply(lambda x: self.lemmatizer(x))\n",
    "\n",
    "        df[col_name] = df[col_name].apply(lambda x: ' '.join(x))\n",
    "\n",
    "        return df\n",
    "\n",
    "    # code adapted from Mimi (2021)\n",
    "    # Bag-of-words using Count Vectorization\n",
    "    @staticmethod\n",
    "    def count_vectorization(df, col_name):\n",
    "        corpus =df[col_name]\n",
    "        vectorizer = CountVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        print(vectorizer.get_feature_names())\n",
    "        return vectorizer, X\n",
    "\n",
    "    # TFIDF Vectorization\n",
    "    @staticmethod\n",
    "    def tfidf_vectorization(df, col_name):\n",
    "        corpus =df[col_name]\n",
    "        vectorizer = TfidfVectorizer()\n",
    "        X = vectorizer.fit_transform(corpus)\n",
    "        return vectorizer, X\n",
    "    # end of adapted code\n",
    "\n",
    "    # https://www.analyticsvidhya.com/blog/2021/06/text-preprocessing-in-nlp-with-python-codes/#:~:text=Text%20preprocessing%20is%20an%20essential,part%2Dof%2Dspeech%20tagging.\n",
    "    # https://www.nltk.org/api/nltk.tokenize.html\n",
    "    # https://www.analyticsvidhya.com/blog/2021/07/bag-of-words-vs-tfidf-vectorization-a-hands-on-tutorial/\n",
    "    # https://www.analyticsvidhya.com/blog/2020/02/quick-introduction-bag-of-words-bow-tf-idf/#:~:text=Bag%20of%20Words%20just%20creates,less%20important%20ones%20as%20well.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
