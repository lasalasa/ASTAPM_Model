{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14cbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_core_utils.ipynb\n",
    "# %run notebook_text_processor.ipynb\n",
    "%run notebook_auto_labeling.ipynb\n",
    "%run notebook_model_ls.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43cd5593",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-09-21 06:09:10.200964: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Text pre-processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "import joblib\n",
    "\n",
    "# from .core_utils import CoreUtils\n",
    "# from .text_processor import TextPreprocessor\n",
    "# from .model_ls import ModelLS\n",
    "\n",
    "# This should be the same as the 'num_words' in the tokenizer\n",
    "MAX_NB_WORDS = 7500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "def pre_process_df(df, col_name):\n",
    "\n",
    "    textPreprocessor = TextPreprocessor()\n",
    "     # Step 1: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    print(\"combined_narrative\")\n",
    "    df = textPreprocessor.combined_narrative(df)\n",
    "    \n",
    "    # print(\"counting_narrative\")\n",
    "    # # Step 2: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    # df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    # print(\"clean_narrative\")\n",
    "    # # Step 3: clean narrative\n",
    "    # df = textPreprocessor.clean_narrative(df)\n",
    "\n",
    "    print(\"clean_feature\")\n",
    "    # Step 4: Clean factor_column_name\n",
    "    df = textPreprocessor.clean_feature(df, col_name)\n",
    "\n",
    "    print(\"drop_narratives\")\n",
    "    # Step 5: Drop column narrative 01 and 02\n",
    "    df = textPreprocessor.drop_narratives(df)\n",
    "\n",
    "    print(\"preprocess_narrative\")\n",
    "    df = textPreprocessor.preprocess_narrative(df)\n",
    "    df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    # Step 6: Show summary\n",
    "    textPreprocessor.show_summary(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, dfs=None, ds_name='asrs', ls_version=1, sample_size=1000, max_length=300, max_nb_words=5000):\n",
    "\n",
    "        self.ds_name = ds_name\n",
    "        self.sample_size = sample_size\n",
    "        self.ls_version = ls_version\n",
    "        self.max_length = max_length\n",
    "        self.max_nb_words = max_nb_words\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"FACTOR_COL_NAME\"]\n",
    "        self.factor_col_name = factor_col_name\n",
    "\n",
    "        if dfs is None:\n",
    "            print(\"The DataFrame is None\")\n",
    "            dfs = self.get_data()        \n",
    "            print(\"Data loaded\")\n",
    "        \n",
    "        # print(dfs[ds_name])\n",
    "\n",
    "        dfs = self.pre_process(dfs)\n",
    "        self.dfs = dfs\n",
    "        print(\"Pre processed\")\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        print(\"Define Y\")\n",
    "        lstm_labels = df['HFACS_Category_Value_Predict'].values\n",
    "        lstm_label_encoder = LabelEncoder()\n",
    "        Y = lstm_label_encoder.fit_transform(lstm_labels)\n",
    "        self.Y = Y\n",
    "        self.label_encoder = lstm_label_encoder\n",
    "        print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "        print(\"Define X\")\n",
    "        texts = df['narrative'].values\n",
    "        tokenizer = Tokenizer(num_words=max_nb_words)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=max_length)\n",
    "        self.X = X\n",
    "\n",
    "        print('Shape of data tensor:', X.shape)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM))\n",
    "        model.add(SpatialDropout1D(0.25))\n",
    "        # model.add(LSTM(units=64, dropout=0.3, recurrent_dropout=0.3))\n",
    "        model.add(LSTM(units=64, return_sequences=True, dropout=0.25, recurrent_dropout=0.25))\n",
    "        model.add(LSTM(units=64, dropout=0.25, recurrent_dropout=0.25))\n",
    "        # model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))  # Add a second LSTM layer\n",
    "        \n",
    "        model.add(Dense(len(lstm_label_encoder.classes_), activation='softmax', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0001),  metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def get_data(self):\n",
    "        ds_name = self.ds_name\n",
    "        ds_name_list = ds_name.split('_')\n",
    "\n",
    "        # print(ds_name_list)\n",
    "        self.sample_size = self.sample_size * len(ds_name_list)\n",
    "\n",
    "        dfs = {}\n",
    "        dfs_list = []\n",
    "        for ds_name_item in ds_name_list:\n",
    "            df_item = CoreUtils.get_data(ds_name_item)\n",
    "            dfs[ds_name_item] = df_item\n",
    "            dfs_list.append(df_item)\n",
    "\n",
    "        dfs[ds_name] = pd.concat(dfs_list, axis=0).reset_index(drop=True)\n",
    "\n",
    "        return dfs\n",
    "    \n",
    "    def pre_process(self, dfs):\n",
    "        ds_name = self.ds_name\n",
    "        sample_size = self.sample_size\n",
    "        ls_version = self.ls_version\n",
    "        factor_col_name = self.factor_col_name\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        # 02. Label Spreading\n",
    "        print(\"Start labelling\")\n",
    "        df = ModelLS.predict(df, ds_name, ls_version, sample_size)\n",
    "        print(\"Ladled Sampling size=\", df.shape)\n",
    "\n",
    "        print(\"start pre_process_df\")\n",
    "        df = pre_process_df(df, factor_col_name)\n",
    "\n",
    "        dfs[ds_name] = df\n",
    "\n",
    "        print(df.head())\n",
    "        return dfs\n",
    "\n",
    "\n",
    "    def train(self, epochs=10, batch_size=32):\n",
    "\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "        print(X_train.shape,Y_train.shape)\n",
    "        print(X_test.shape,Y_test.shape)\n",
    "\n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "            patience=2, \n",
    "            min_delta=0.001)\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(X_train, Y_train, \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1, \n",
    "            callbacks=[early_stopping])\n",
    "        \n",
    "        self.history = history\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "\n",
    "        self.save()\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Predict the probabilities for the test set\n",
    "        model = self.model\n",
    "        ds_name = self.ds_name\n",
    "        X = self.X\n",
    "        Y_test = self.Y_test\n",
    "        X_test = self.X_test\n",
    "        labels = self.label_encoder.classes_\n",
    "\n",
    "        map_labels = self.label_encoder.fit_transform(labels)\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Convert probabilities to class indices\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        conf_matrix = confusion_matrix(Y_test, y_pred_classes, labels=map_labels)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(Y_test, y_pred_classes)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "        \n",
    "        report = classification_report(Y_test, y_pred_classes, labels=map_labels, output_dict=True)\n",
    "\n",
    "        # Classification report for precision, recall, F1-score\n",
    "        print(classification_report(Y_test, y_pred_classes, labels=map_labels))\n",
    "\n",
    "        print(conf_matrix)\n",
    "\n",
    "        history = self.history\n",
    "\n",
    "        train_loss = history.history['loss']\n",
    "        test_loss = history.history['val_loss']\n",
    "\n",
    "        train_accuracy = history.history['accuracy']\n",
    "        test_accuracy = history.history['val_accuracy']\n",
    "\n",
    "        print(train_loss, test_loss)\n",
    "\n",
    "        df = self.dfs[ds_name]\n",
    "\n",
    "        train_data_label_value_count = df['HFACS_Category_Value_Predict'].value_counts().reset_index()\n",
    "\n",
    "        # Convert DataFrame to dictionary or list of records\n",
    "        train_data_label_value_count_dict = train_data_label_value_count.to_dict(orient='records')\n",
    "\n",
    "        # https://docs.python.org/3/library/collections.html#collections.Counter\n",
    "        all_words = [word for text in df['narrative'].values for word in text.split()]\n",
    "        word_counts = Counter(all_words)\n",
    "        common_words = word_counts.most_common(50)\n",
    "        word_count_labels, word_count_values = zip(*common_words)\n",
    "\n",
    "        narrative_word_count = df['narrative_word_count'].tolist()\n",
    "\n",
    "        predict_text = model.predict(X)\n",
    "        # Convert probabilities to class indices\n",
    "        predict_text_classes = np.argmax(predict_text, axis=1)\n",
    "\n",
    "        predict_text_label = self.label_encoder.inverse_transform(predict_text_classes)\n",
    "\n",
    "        df['lstm_predict_label'] = predict_text_label\n",
    "\n",
    "        print(df['date'])\n",
    "        \n",
    "        new_df = df.groupby('date')['lstm_predict_label'].value_counts().unstack().fillna(0)\n",
    "\n",
    "        evaluation_result = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"classification_report\": report,\n",
    "            \"conf_matrix\": conf_matrix.tolist(),\n",
    "            \"labels\": labels.tolist(),\n",
    "            \"train_loss\": train_loss,\n",
    "            \"test_loss\": test_loss,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"test_accuracy\": test_accuracy,\n",
    "            \"train_label_value_count\": train_data_label_value_count_dict,\n",
    "            \"train_word_count\": {\n",
    "                \"labels\": word_count_labels,\n",
    "                \"values\": word_count_values\n",
    "            },\n",
    "            \"train_narrative_word_count\": narrative_word_count,\n",
    "            \"sample_predict_view\": new_df.to_dict(orient=\"index\")\n",
    "        }\n",
    "\n",
    "        # print(evaluation_result)\n",
    "\n",
    "        return evaluation_result\n",
    "\n",
    "    def predict(self, text):\n",
    "        loaded_model = self.load()\n",
    "        predict_text = loaded_model.predict(text)\n",
    "        predict_text_classes = np.argmax(predict_text, axis=1)\n",
    "        return predict_text_classes\n",
    "    \n",
    "    def save(self):\n",
    "        ds_name = self.ds_name\n",
    "        self.model.save(f'{ds_name}.keras')\n",
    "    \n",
    "    def load(self):\n",
    "        ds_name = self.ds_name\n",
    "        loaded_model = load_model(f'{ds_name}.keras')\n",
    "        loaded_model.summary()\n",
    "        return loaded_model\n",
    "\n",
    "# https://www.kaggle.com/code/khotijahs1/using-lstm-for-nlp-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
