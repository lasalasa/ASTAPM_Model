{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b14cbbf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run notebook_core_utils.ipynb\n",
    "# %run notebook_text_processor.ipynb\n",
    "%run notebook_auto_labeling.ipynb\n",
    "%run notebook_model_ls.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43cd5593",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Text pre-processing\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.layers import LSTM, GRU, Dense, Embedding, Dropout, GlobalAveragePooling1D, Flatten, SpatialDropout1D, Bidirectional\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from tensorflow.keras.regularizers import l2\n",
    "# from keras.optimizers import Adam\n",
    "# from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import joblib\n",
    "\n",
    "# from .core_utils import CoreUtils\n",
    "# from .text_processor import TextPreprocessor\n",
    "# from .model_ls import ModelLS\n",
    "\n",
    "# This should be the same as the 'num_words' in the tokenizer\n",
    "MAX_NB_WORDS = 7500\n",
    "# This is fixed.\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "MAX_LENGTH = 300\n",
    "\n",
    "## Define X\n",
    "# def get_LSTM_X(df, max_length=300):\n",
    "\n",
    "#     # The maximum number of words to be used. (most frequent)\n",
    "    \n",
    "#     texts = df['narrative'].values\n",
    "\n",
    "#     tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "#     tokenizer.fit_on_texts(texts)\n",
    "\n",
    "#     word_index = tokenizer.word_index\n",
    "#     print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "#     sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "#     X = pad_sequences(sequences, maxlen=max_length)\n",
    "\n",
    "#     print('Shape of data tensor:', X.shape)\n",
    "#     return X\n",
    "\n",
    "## Define Y\n",
    "\n",
    "# def get_LSTM_Y(df):\n",
    "#     lstm_labels = df['HFACS_Category_Value_Predict'].values\n",
    "#     lstm_label_encoder = LabelEncoder()\n",
    "#     Y = lstm_label_encoder.fit_transform(lstm_labels)\n",
    "#     print('Shape of label tensor:', Y.shape)\n",
    "#     return lstm_label_encoder, Y\n",
    "\n",
    "# def get_X_Y_train_test(df):\n",
    "#     ## Text clean\n",
    "#     clean_df = df.reset_index(drop=True)\n",
    "#     clean_df['narrative'] = clean_df['narrative'].apply(clean_text)\n",
    "#     clean_df['narrative'] = clean_df['narrative'].str.replace('\\d+', '')\n",
    "\n",
    "#     ## Tokenize\n",
    "#     # # Defining pre-processing parameters\n",
    "#     # max_len = 10000 \n",
    "#     # trunc_type = 'post'\n",
    "#     # padding_type = 'post'\n",
    "#     # oov_tok = '<OOV>' # out of vocabulary token\n",
    "#     # vocab_size = 500\n",
    "\n",
    "#     X = get_LSTM_X(clean_df, max_length=250)\n",
    "\n",
    "#     lstm_label_encoder, Y = get_LSTM_Y(clean_df)\n",
    "#     df['HFACS_Category_Value_Predict_Encode'] = Y\n",
    "\n",
    "#     ## Define Train & Test\n",
    "#     X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "#     print(X_train.shape,Y_train.shape)\n",
    "#     print(X_test.shape,Y_test.shape)\n",
    "#     return lstm_label_encoder, X, Y, X_train, X_test, Y_train, Y_test\n",
    "\n",
    "def pre_process_df(df, col_name):\n",
    "\n",
    "    textPreprocessor = TextPreprocessor()\n",
    "     # Step 1: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    print(\"combined_narrative\")\n",
    "    df = textPreprocessor.combined_narrative(df)\n",
    "    \n",
    "    print(\"counting_narrative\")\n",
    "    # Step 2: Combine 'narrative_01' and 'narrative_02' into a single 'narrative' column\n",
    "    df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    print(\"clean_narrative\")\n",
    "    # Step 3: clean narrative\n",
    "    df = textPreprocessor.clean_narrative(df)\n",
    "\n",
    "    print(\"clean_feature\")\n",
    "    # Step 4: Clean factor_column_name\n",
    "    df = textPreprocessor.clean_feature(df, col_name)\n",
    "\n",
    "    print(\"drop_narratives\")\n",
    "    # Step 5: Drop column narrative 01 and 02\n",
    "    df = textPreprocessor.drop_narratives(df)\n",
    "\n",
    "    print(\"preprocess_narrative\")\n",
    "    df = textPreprocessor.preprocess_narrative(df)\n",
    "    df = textPreprocessor.counting_narrative(df)\n",
    "\n",
    "    # Step 6: Show summary\n",
    "    textPreprocessor.show_summary(df)\n",
    "\n",
    "    return df\n",
    "\n",
    "class LSTMModel:\n",
    "    def __init__(self, dfs=None, ds_name='asrs', sample_size=1000):\n",
    "\n",
    "        self.ds_name = ds_name\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"FACTOR_COL_NAME\"]\n",
    "        self.factor_col_name = factor_col_name\n",
    "\n",
    "        if dfs is None:\n",
    "            print(\"The DataFrame is None\")\n",
    "            dfs = self.get_data()        \n",
    "            print(\"Data loaded\")\n",
    "\n",
    "        dfs = self.pre_process(dfs)\n",
    "        self.dfs = dfs\n",
    "        print(\"Pre processed\")\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        print(\"Define Y\")\n",
    "        lstm_labels = df['HFACS_Category_Value_Predict'].values\n",
    "        lstm_label_encoder = LabelEncoder()\n",
    "        Y = lstm_label_encoder.fit_transform(lstm_labels)\n",
    "        self.Y = Y\n",
    "        print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "        print(\"Define X\")\n",
    "        texts = df['narrative'].values\n",
    "        tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "        tokenizer.fit_on_texts(texts)\n",
    "        word_index = tokenizer.word_index\n",
    "        print('Found %s unique tokens.' % len(word_index))\n",
    "        sequences = tokenizer.texts_to_sequences(texts)\n",
    "        X = pad_sequences(sequences, maxlen=MAX_LENGTH)\n",
    "        self.X = X\n",
    "\n",
    "        print('Shape of data tensor:', X.shape)\n",
    "        \n",
    "        model = Sequential()\n",
    "        model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=X.shape[1]))\n",
    "        model.add(SpatialDropout1D(0.3))\n",
    "        # model.add(LSTM(units=64, dropout=0.3, recurrent_dropout=0.3))\n",
    "        model.add(LSTM(units=64, return_sequences=True, dropout=0.3, recurrent_dropout=0.3))\n",
    "        model.add(LSTM(units=64, dropout=0.3, recurrent_dropout=0.3))\n",
    "        # model.add(LSTM(units=64, dropout=0.2, recurrent_dropout=0.2))  # Add a second LSTM layer\n",
    "        \n",
    "        model.add(Dense(len(lstm_label_encoder.classes_), activation='softmax', kernel_regularizer=l2(0.001)))\n",
    "\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=Adam(learning_rate=0.0001),  metrics=['accuracy'])\n",
    "\n",
    "        self.model = model\n",
    "\n",
    "    def get_data(self):\n",
    "        ds_name = self.ds_name\n",
    "        ds_name_list = ds_name.split('_')\n",
    "\n",
    "        print(ds_name_list)\n",
    "        dfs = {}\n",
    "\n",
    "        for ds_name_item in ds_name_list:\n",
    "            df_item = CoreUtils.get_data(ds_name_item)\n",
    "            dfs[ds_name_item] = df_item\n",
    "        \n",
    "        return dfs\n",
    "    \n",
    "    def pre_process(self, dfs):\n",
    "        ds_name = self.ds_name\n",
    "        sample_size = self.sample_size\n",
    "        factor_col_name = self.factor_col_name\n",
    "\n",
    "        df = dfs[ds_name]\n",
    "\n",
    "        # 02. Label Spreading\n",
    "        print(\"start labelling\")\n",
    "        modelLS = ModelLS(dfs, ds_name)\n",
    "        df = modelLS.predict(df, sample_size)\n",
    "        print(\"Sampling size\", df.shape)\n",
    "\n",
    "        print(\"start pre_process_df\")\n",
    "        df = pre_process_df(df, factor_col_name)\n",
    "\n",
    "        dfs[ds_name] = df\n",
    "        return dfs\n",
    "\n",
    "\n",
    "    def train(self, epochs=10, batch_size=32):\n",
    "\n",
    "        X = self.X\n",
    "        Y = self.Y\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.10, random_state = 42)\n",
    "        print(X_train.shape,Y_train.shape)\n",
    "        print(X_test.shape,Y_test.shape)\n",
    "\n",
    "        # Early stopping to prevent overfitting\n",
    "        early_stopping = EarlyStopping(monitor='val_loss', \n",
    "            patience=2, \n",
    "            min_delta=0.001)\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(X_train, Y_train, \n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            validation_split=0.1, \n",
    "            callbacks=[early_stopping])\n",
    "        \n",
    "        self.history = history\n",
    "        self.X_train = X_train\n",
    "        self.X_test = X_test\n",
    "        self.Y_train = Y_train\n",
    "        self.Y_test = Y_test\n",
    "\n",
    "        self.save()\n",
    "\n",
    "\n",
    "    def evaluate(self):\n",
    "        # Predict the probabilities for the test set\n",
    "        model = self.model\n",
    "        Y_test = self.Y_test\n",
    "        X_test = self.X_test\n",
    "\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "        # Convert probabilities to class indices\n",
    "        y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "        # Accuracy\n",
    "        accuracy = accuracy_score(Y_test, y_pred_classes)\n",
    "        print(f'Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        # Classification report for precision, recall, F1-score\n",
    "        print(classification_report(Y_test, y_pred_classes))\n",
    "\n",
    "    def predict(self, text):\n",
    "        loaded_model = self.load()\n",
    "        predict_text = loaded_model.predict(text)\n",
    "        predict_text_classes = np.argmax(predict_text, axis=1)\n",
    "        return predict_text_classes\n",
    "    \n",
    "    def save(self):\n",
    "        ds_name = self.ds_name\n",
    "        self.model.save(f'{ds_name}.keras')\n",
    "    \n",
    "    def load(self):\n",
    "        ds_name = self.ds_name\n",
    "        loaded_model = load_model(f'{ds_name}.keras')\n",
    "        loaded_model.summary()\n",
    "        return loaded_model\n",
    "\n",
    "# https://www.kaggle.com/code/khotijahs1/using-lstm-for-nlp-text-classification"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
