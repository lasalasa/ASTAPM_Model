{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "25d94245",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/lasanthalakmal/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/lasanthalakmal/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%run notebook_core_utils.ipynb\n",
    "%run notebook_text_processor.ipynb\n",
    "%run notebook_labeling_auto.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c6dd4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "803b88a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "class OsOperation:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def create_folder(self, folder_path):\n",
    "        try:\n",
    "            os.makedirs(folder_path, exist_ok=True)\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred (create_folder): {e}\")\n",
    "\n",
    "    # https://www.geeksforgeeks.org/python-loop-through-folders-and-files-in-directory/\n",
    "    def get_dir_files(self, folder_path):\n",
    "        try:\n",
    "            file_name_list = os.listdir(folder_path)\n",
    "            sorted_file_name_list = sorted(file_name_list)\n",
    "            return sorted_file_name_list\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred (listdir_files): {e}\")\n",
    "\n",
    "    def get_dir_unique_path(self, folder_path):\n",
    "\n",
    "        unique_name = datetime.now().strftime('%Y%m%d%H%M%S')\n",
    "\n",
    "        unique_path = os.path.join(folder_path, unique_name)\n",
    "\n",
    "        return unique_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd5d7b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# from src.core.core_utils import CoreUtils\n",
    "# from src.core.text_processor import TextPreprocessor\n",
    "# from src.core.labeling_auto import AutoLabeling, show_disctribution\n",
    "# from src.extensions.os_extension import OsOperation\n",
    "\n",
    "#----------- Model Save/Load Func ----------\n",
    "def save_model_LS(model, vectorizer, label_encoder, unmapped_encoder, name):\n",
    "    # Save both the model and vectorizer to a file\n",
    "\n",
    "    PATH_PREFIX = f'../data/model_store/{name}'\n",
    "\n",
    "    osOperation = OsOperation()\n",
    "    file_list = osOperation.get_dir_files(PATH_PREFIX)\n",
    "\n",
    "    next_version = len(file_list) + 1\n",
    "\n",
    "    dump_name = f'{PATH_PREFIX}/model_LS_{name}_{next_version}.pkl'\n",
    "    \n",
    "    # code adapted from Sharma (2023)\n",
    "    joblib.dump((model, vectorizer, label_encoder, unmapped_encoder), dump_name)\n",
    "    # end of adapted code\n",
    "    print(f\"{name} Model and vectorizer saved successfully\")\n",
    "\n",
    "def load_model_LS(name, version=0):\n",
    "    # Load both the model and vectorizer from the file\n",
    "\n",
    "    PATH_PREFIX = f'../data/model_store/{name}'\n",
    "\n",
    "    if version == 0:\n",
    "        dump_name = f'{PATH_PREFIX}/model_LS_{name}_1.pkl'\n",
    "    else:\n",
    "        dump_name = f'{PATH_PREFIX}/model_LS_{name}_{version}.pkl'\n",
    "    \n",
    "    # code adapted from Sharma (2023)\n",
    "    model, vectorizer,label_encoder, unmapped_encoder = joblib.load(dump_name)\n",
    "    # end of adapted code\n",
    "    print(\"Model and vectorizer loaded successfully\")\n",
    "    return model, vectorizer, label_encoder, unmapped_encoder\n",
    "#----------- Model Save/Load Func ----------\n",
    "\n",
    " #----------- Visualization Func ----------\n",
    "# Show Label Distribution\n",
    "def show_label(df, label_col):\n",
    "    plt.figure(figsize=(8,6))\n",
    "    sns.countplot(df[label_col])\n",
    "    plt.title('The distribution of Primary problem')\n",
    "\n",
    "# confusion_matrix\n",
    "def show_confusion_matrix(model, X_test, y_test, label_encoder):\n",
    "    # Predict the classes for the test set\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    label_classes = label_encoder.classes_\n",
    "    all_labels = label_encoder.fit_transform(label_classes)\n",
    "\n",
    "    # Generate the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred, labels=all_labels)\n",
    "\n",
    "    # Plot the confusion matrix\n",
    "    plt.figure(figsize=(10,8))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=label_classes, yticklabels=label_classes)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "#----------- Visualization Func ----------\n",
    "\n",
    "class ModelLS:\n",
    "\n",
    "    def __init__(self, dfs=None, ds_name='asrs', sample_size=0, options={\n",
    "        \"is_merge_taxonomy\": False\n",
    "    }):\n",
    "\n",
    "        self.ds_name = ds_name\n",
    "        self.sample_size = sample_size\n",
    "        self.options = options\n",
    "\n",
    "        if options['is_merge_taxonomy']:\n",
    "            category_value = 'HFACS_Category_balance_Value'\n",
    "        else:\n",
    "            category_value = 'HFACS_Category_Value'\n",
    "\n",
    "        self.category_value = category_value\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"LS_CLASSIFICATION_FACTOR\"]\n",
    "        self.factor_col_name = factor_col_name\n",
    "\n",
    "        if dfs is None:\n",
    "            print(\"The DataFrame is None\")\n",
    "            dfs = self.get_data()\n",
    "        \n",
    "        self.dfs = dfs\n",
    "\n",
    "        df_labeled = self.do_label()\n",
    "\n",
    "        df_labeled = df_labeled.dropna(subset=[category_value, factor_col_name])\n",
    "\n",
    "        labels = df_labeled[category_value].values\n",
    "        label_encoder = LabelEncoder()\n",
    "        Y = label_encoder.fit_transform(labels)\n",
    "\n",
    "        print('Shape of label tensor:', Y.shape)\n",
    "\n",
    "        self.df_labeled = df_labeled\n",
    "        self.label_encoder = label_encoder\n",
    "        self.Y = Y\n",
    "\n",
    "    def get_data(self):\n",
    "        ds_name = self.ds_name\n",
    "        ds_name_list = ds_name.split('_')\n",
    "\n",
    "        dfs = {}\n",
    "\n",
    "        for ds_name_item in ds_name_list:\n",
    "            df_item = CoreUtils.get_data(ds_name_item)\n",
    "            dfs[ds_name_item] = df_item\n",
    "        \n",
    "        return dfs\n",
    "\n",
    "    def do_label(self):\n",
    "        sample_size = self.sample_size\n",
    "\n",
    "        ds_name = self.ds_name\n",
    "        dfs = self.dfs.copy()\n",
    "\n",
    "        ds_name_list = ds_name.split('_')\n",
    "\n",
    "        if len(ds_name_list) > 1:\n",
    "            labeled_dfs = []\n",
    "            ds_name_item_01 = ds_name_list[0]\n",
    "            df = dfs[ds_name_item_01]\n",
    "            \n",
    "            autoLabeling_01 = AutoLabeling(df)\n",
    "            df_labeled = autoLabeling_01.do_auto_label(sample_size)\n",
    "            labeled_dfs.append(df_labeled)\n",
    "\n",
    "            ds_name_list.pop(0)\n",
    "\n",
    "            for ds_name_item in ds_name_list:\n",
    "                df_next = dfs[ds_name_item]\n",
    "                autoLabeling = AutoLabeling(df_next)\n",
    "                df_labeled_next = autoLabeling.do_auto_label(sample_size)\n",
    "                labeled_dfs.append(df_labeled_next)\n",
    "\n",
    "            merged_df = pd.concat(labeled_dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "            show_disctribution(merged_df)\n",
    "            \n",
    "            return merged_df\n",
    "\n",
    "        \n",
    "\n",
    "        df = dfs[ds_name]\n",
    "        autoLabeling = AutoLabeling(df)\n",
    "        df_labeled = autoLabeling.do_auto_label(sample_size)\n",
    "        return df_labeled\n",
    "\n",
    "    # Model Train\n",
    "    def train(self):\n",
    "\n",
    "        ds_name = self.ds_name\n",
    "        ls_df = self.df_labeled.copy()\n",
    "        factor_column_name = self.factor_col_name\n",
    "\n",
    "        # print(self.Y)\n",
    "\n",
    "        label_encoder, y = [self.label_encoder, self.Y] \n",
    "        unmapped_encoder = label_encoder.transform(['Unmapped'])[0]\n",
    "\n",
    "        # Set unlabeled data points to -1 (required by LabelSpreading)\n",
    "        y[y == unmapped_encoder] = -1\n",
    "\n",
    "        # print(unmapped_encoder, y)\n",
    "\n",
    "        # Vectorize the 'Combined_Factors' column using TF-IDF\n",
    "        textPreprocessor = TextPreprocessor()\n",
    "        vectorizer, X,  = textPreprocessor.tfidf_vectorization(ls_df, factor_column_name) \n",
    "\n",
    "        # Split the labeled data into train and test sets (for evaluation)\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X[y != -1], y[y != -1], test_size=0.2, random_state=42)\n",
    "\n",
    "        # Create and fit the LabelSpreading model using ALL data (labeled + unlabeled)\n",
    "        label_prop_model = LabelSpreading()\n",
    "        label_prop_model.fit(X, y)\n",
    "\n",
    "        # Make predictions on the test set (only labeled data for evaluation)\n",
    "        y_test_predict = label_prop_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_test_predict)\n",
    "        print(\"Accuracy on test data:\", accuracy)\n",
    "\n",
    "        # Optionally show confusion matrix\n",
    "        show_confusion_matrix(label_prop_model, X_test, y_test, label_encoder)\n",
    "\n",
    "        # Predict the labels for the full dataset (including unlabeled data)\n",
    "        y_full_predict = label_prop_model.predict(X)\n",
    "\n",
    "        # Handle -1 predictions by replacing them with 'Unmapped'\n",
    "        y_full_predict = np.where(y_full_predict == -1, unmapped_encoder, y_full_predict)\n",
    "\n",
    "        # Decode the predicted labels back to their original categories\n",
    "        y_decoded = label_encoder.inverse_transform(y_full_predict)\n",
    "\n",
    "        # Add the predicted labels back to the DataFrame\n",
    "        ls_df['HFACS_Category_Value_Predict'] = y_decoded\n",
    "\n",
    "        # Save Model/Vectorizer and label_encoder\n",
    "        save_model_LS(label_prop_model, vectorizer, label_encoder, unmapped_encoder, ds_name)\n",
    "\n",
    "    # Model predict function\n",
    "    @staticmethod\n",
    "    def predict(df: pd.DataFrame, model_name='asrs_ntsb', version=0, sample_size=0):\n",
    "\n",
    "        factor_col_name = CoreUtils.get_constant()[\"LS_CLASSIFICATION_FACTOR\"]\n",
    "\n",
    "        model, vectorizer, label_encoder, unmapped_encoder = load_model_LS(model_name, version)\n",
    "\n",
    "        # New set of data for prediction\n",
    "        print(\"LS sample_size====\", sample_size)\n",
    "        if sample_size > 0:\n",
    "            labeled_data =  df.tail(sample_size).copy()\n",
    "        else:\n",
    "            labeled_data =  df\n",
    "\n",
    "        print(\"Factors Null count\", labeled_data[factor_col_name].isnull().sum())\n",
    "\n",
    "        # Drop rows with NaN in 'finding_factors'\n",
    "        labeled_data = labeled_data.dropna(subset=[factor_col_name])\n",
    "\n",
    "        # Vectorize the new set of data\n",
    "        X_vec = vectorizer.transform(labeled_data[factor_col_name])  \n",
    "\n",
    "        # Predict using the trained LabelSpreading model\n",
    "        y = model.predict(X_vec)\n",
    "\n",
    "        # Replace -1 values with 'Unmapped' placeholder\n",
    "        y = np.where(y == -1, unmapped_encoder, y)\n",
    "\n",
    "        # Decode the predicted labels\n",
    "        y_decoded = label_encoder.inverse_transform(y)\n",
    "\n",
    "        # Add predictions to the new DataFrame\n",
    "        labeled_data = labeled_data.copy() \n",
    "        labeled_data['HFACS_Category_Value_Predict'] = y_decoded\n",
    "\n",
    "        return labeled_data\n",
    "\n",
    "\n",
    "    # https://www.analyticsvidhya.com/blog/2023/02/how-to-save-and-load-machine-learning-models-in-python-using-joblib-library/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
